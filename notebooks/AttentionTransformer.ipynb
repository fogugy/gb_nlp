{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В предыдущей серии\n",
    "\n",
    "\n",
    "<img src=\"images/RNNCompar.png\"/>\n",
    "\n",
    "\n",
    "Мы посмотрели на задачу классификации текстов. Но есть ряд более сильных подходов, которые лучше показывать через задачу генерации\n",
    "\n",
    "\n",
    "# Генерация текстов, encoder-decoder\n",
    "\n",
    "<img src=\"images/EncDec.png\"/>\n",
    "\n",
    "\n",
    "Данная архитектура называется seq2seq, простыми словами выглядит она следующим образом:\n",
    "<img src=\"images/seq2seq.png\"/>\n",
    "\n",
    "\n",
    "эту модель можно строить на уровне слов и на уровне токенов. Попробуем обучить на уровне токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'data/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kinetik/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/kinetik/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kinetik/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 1.3176 - accuracy: 0.7205 - val_loss: 1.0990 - val_accuracy: 0.7055\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.8840 - accuracy: 0.7547 - val_loss: 0.9064 - val_accuracy: 0.7549\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.7405 - accuracy: 0.7971 - val_loss: 0.7782 - val_accuracy: 0.7813\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.6432 - accuracy: 0.8179 - val_loss: 0.7026 - val_accuracy: 0.7986\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.5881 - accuracy: 0.8310 - val_loss: 0.6621 - val_accuracy: 0.8072\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.5518 - accuracy: 0.8405 - val_loss: 0.6222 - val_accuracy: 0.8198\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.5243 - accuracy: 0.8474 - val_loss: 0.5978 - val_accuracy: 0.8255\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.5032 - accuracy: 0.8528 - val_loss: 0.5855 - val_accuracy: 0.8284\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4851 - accuracy: 0.8573 - val_loss: 0.5658 - val_accuracy: 0.8324\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4684 - accuracy: 0.8620 - val_loss: 0.5509 - val_accuracy: 0.8381\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.4534 - accuracy: 0.8660 - val_loss: 0.5457 - val_accuracy: 0.8388\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4403 - accuracy: 0.8700 - val_loss: 0.5362 - val_accuracy: 0.8421\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.4278 - accuracy: 0.8734 - val_loss: 0.5198 - val_accuracy: 0.8471\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4160 - accuracy: 0.8768 - val_loss: 0.5110 - val_accuracy: 0.8497\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.4047 - accuracy: 0.8798 - val_loss: 0.5026 - val_accuracy: 0.8520\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3937 - accuracy: 0.8828 - val_loss: 0.4969 - val_accuracy: 0.8534\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3837 - accuracy: 0.8858 - val_loss: 0.4885 - val_accuracy: 0.8565\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3733 - accuracy: 0.8888 - val_loss: 0.4841 - val_accuracy: 0.8574\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3632 - accuracy: 0.8918 - val_loss: 0.4785 - val_accuracy: 0.8590\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3535 - accuracy: 0.8942 - val_loss: 0.4774 - val_accuracy: 0.8591\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3451 - accuracy: 0.8968 - val_loss: 0.4668 - val_accuracy: 0.8629\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3351 - accuracy: 0.8998 - val_loss: 0.4661 - val_accuracy: 0.8635\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.3266 - accuracy: 0.9023 - val_loss: 0.4638 - val_accuracy: 0.8649\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3177 - accuracy: 0.9050 - val_loss: 0.4584 - val_accuracy: 0.8652\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.3094 - accuracy: 0.9071 - val_loss: 0.4567 - val_accuracy: 0.8666\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.4081 - accuracy: 0.8799 - val_loss: 0.4746 - val_accuracy: 0.8611\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.3311 - accuracy: 0.9010 - val_loss: 0.4582 - val_accuracy: 0.8662\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.3108 - accuracy: 0.9072 - val_loss: 0.4538 - val_accuracy: 0.8679\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.2982 - accuracy: 0.9107 - val_loss: 0.4508 - val_accuracy: 0.8687\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.2893 - accuracy: 0.9136 - val_loss: 0.4487 - val_accuracy: 0.8701\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Courez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Sais-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au le !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Aide-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Coutez !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Vas-y.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Vas-y.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Vas-y.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Ailez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Ailez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je l'ai les maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'ai parti.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai re le moir !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai re le moir !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je le mende.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Est-ce que je toi !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attandez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attandez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À la chant !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À la chant !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À la chant !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À la chant !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Arrez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Ales-tu ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Ales-tu ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Ales-tu ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Monte.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Monte.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Ais-y !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Ais-y !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis demande an souri.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis demande an souri.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je laisonne.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: J'ai paril ma chante.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: J'ai paril ma chante.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: J'ai une mien.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai parti mal.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: J'ai anturi.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: J'ai une mienne.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis des armien.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis des armien.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: La soyez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas un pour !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: At rappes ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: At rappes ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: At rappes ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Prends pas !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous sommes des soures.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons tous de moi.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons tous de moi.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons tous de moi.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons tous de moi.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande-le.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Soyez césicite !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calit !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calit !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calit !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calit !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calit !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calit !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Soyez à chez vous !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Alle !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Venez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но есть проблемы:\n",
    "- на длинных последовательностях результат будет не очень - быстро забывается контекст\n",
    "- хочется научить сеть смотреть в определенное место в прошлом при генерации\n",
    "\n",
    "attention\n",
    "\n",
    "<img src=\"images/Attention.png\"/>\n",
    "\n",
    "<img src=\"images/Attention2.png\"/>\n",
    "\n",
    "\n",
    "- h(t): скрытое состояние декодера\n",
    "- c(t): вектор контекста, который подается на вход\n",
    "- y(t): текущий таргет\n",
    "- $\\bar{h}(t)$: скрытое состояние attention\n",
    "- a(t): скор нормализации\n",
    "\n",
    "\n",
    "$$\\bar{h}(t)\\ =\\ tanh(W_c\\ [c_t,\\ h_t]) $$\n",
    "\n",
    "$$P(y_t|y_{<t},\\ x)\\ =\\ softmax(W_s\\ \\bar{h}_t) $$\n",
    "\n",
    "\n",
    "Зачем нужен скор нормализации? - пытаемся сравнить похожесть текущего скрытого состояния и скрытого состояния из прошлого и понять, на что обращать внимание\n",
    "\n",
    "\n",
    "$$a_t(s)\\ =\\ \\frac{exp(score(h_t,\\ \\bar{h}_s))}{\\sum_{i}\\ exp(score(h_t,\\ \\bar{h}_i)) }$$ \n",
    "\n",
    "\n",
    "<img src=\"images/scores.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поднимемся на уровень слов, чтобы можно было что-то более адекватное посчитать за адекватное время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "data_path = 'data/fra-eng/fra.txt'\n",
    "num_samples = 10000\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kinetik/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1 Loss 1.6334\n",
      "Epoch 2 Loss 1.1838\n",
      "Epoch 3 Loss 0.9790\n",
      "Epoch 4 Loss 0.8312\n",
      "Epoch 5 Loss 0.7131\n",
      "Epoch 6 Loss 0.6077\n",
      "Epoch 7 Loss 0.5167\n",
      "Epoch 8 Loss 0.4360\n",
      "Epoch 9 Loss 0.3691\n",
      "Epoch 10 Loss 0.3080\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые украденные функции для оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> good morning <end>\n",
      "Predicted translation: sois gentil ! <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinetik/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ticker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c62fe7b3b252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pylab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'good morning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-61331b77bd12>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-61331b77bd12>\u001b[0m in \u001b[0;36mplot_attention\u001b[0;34m(attention, sentence, predicted_sentence)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredicted_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ticker' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<img src=\"images/transformer.png\"/>\n",
    "\n",
    "\n",
    "Идея в том, что каждое слово параллельно проходит через слои, изображенные на картинке.\n",
    "Некоторые из них — это стандартные fully-connected layers, некоторые — shortcut connections как в ResNet (там, где на картинке Add).\n",
    "\n",
    "\n",
    "Multi-head attention - это специальный новый слой, который дает возможность каждому входному вектору взаимодействовать с другими словами через attention mechanism, вместо передачи hidden state как в RNN или соседних слов как в CNN.\n",
    "\n",
    "\n",
    "<img src=\"images/mha.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/AttTr.png\"/>\n",
    "\n",
    "\n",
    "Работа энкодера:\n",
    "\n",
    "\n",
    "Делаются эмбеддинги для всех слов предложения (вектора одинаковой размерности). Для примера пусть это будет предложение I am stupid. В эмбеддинг добавляется еще позиция слова в предложении.\n",
    "\n",
    "\n",
    "Берется вектор первого слова и вектор второго слова (I, am), подаются на однослойную сеть с одним выходом, которая выдает степень их похожести (скалярная величина). Эта скалярная величина умножается на вектор второго слова, получая его некоторую \"ослабленную\" на величину похожести копию.\n",
    "\n",
    "\n",
    "Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов I, stupid).\n",
    "\n",
    "\n",
    "Делая тоже самое для всех оставшихся слов предложения получаются их \"ослабленные\" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга:\n",
    "output=am * weight(I, am) + stupid * weight(I, stupid)\n",
    "\n",
    "\n",
    "Это механизм \"обычного\" attention.\n",
    "Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. Типа одна один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.\n",
    "\n",
    "\n",
    "На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (I). Конкантенируем этот вректор в один.\n",
    "\n",
    "\n",
    "Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.\n",
    "\n",
    "\n",
    "Такой же процесс производится для всех других слов в предложении.\n",
    "\n",
    "\n",
    "Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.2704 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.1042 Accuracy 0.0148\n",
      "Epoch 1 Batch 100 Loss 7.8725 Accuracy 0.0404\n",
      "Epoch 1 Loss 7.7805 Accuracy 0.0455\n",
      "Epoch 2 Batch 0 Loss 7.2858 Accuracy 0.0667\n",
      "Epoch 2 Batch 50 Loss 7.1240 Accuracy 0.0667\n",
      "Epoch 2 Batch 100 Loss 6.9004 Accuracy 0.0667\n",
      "Epoch 2 Loss 6.7806 Accuracy 0.0667\n",
      "Epoch 3 Batch 0 Loss 6.0503 Accuracy 0.0667\n",
      "Epoch 3 Batch 50 Loss 5.7819 Accuracy 0.1003\n",
      "Epoch 3 Batch 100 Loss 5.5084 Accuracy 0.1095\n",
      "Epoch 3 Loss 5.3948 Accuracy 0.1131\n",
      "Epoch 4 Batch 0 Loss 4.7114 Accuracy 0.1333\n",
      "Epoch 4 Batch 50 Loss 4.5756 Accuracy 0.1387\n",
      "Epoch 4 Batch 100 Loss 4.4015 Accuracy 0.1455\n",
      "Epoch 4 Loss 4.3359 Accuracy 0.1478\n",
      "Epoch 5 Batch 0 Loss 3.8642 Accuracy 0.1708\n",
      "Epoch 5 Batch 50 Loss 3.8154 Accuracy 0.1682\n",
      "Epoch 5 Batch 100 Loss 3.7026 Accuracy 0.1732\n",
      "Epoch 5 Loss 3.6671 Accuracy 0.1746\n",
      "Epoch 6 Batch 0 Loss 3.3714 Accuracy 0.1917\n",
      "Epoch 6 Batch 50 Loss 3.3321 Accuracy 0.1888\n",
      "Epoch 6 Batch 100 Loss 3.2596 Accuracy 0.1922\n",
      "Epoch 6 Loss 3.2400 Accuracy 0.1932\n",
      "Epoch 7 Batch 0 Loss 3.0240 Accuracy 0.2042\n",
      "Epoch 7 Batch 50 Loss 3.0168 Accuracy 0.2030\n",
      "Epoch 7 Batch 100 Loss 2.9630 Accuracy 0.2055\n",
      "Epoch 7 Loss 2.9515 Accuracy 0.2059\n",
      "Epoch 8 Batch 0 Loss 2.7849 Accuracy 0.2156\n",
      "Epoch 8 Batch 50 Loss 2.7892 Accuracy 0.2118\n",
      "Epoch 8 Batch 100 Loss 2.7336 Accuracy 0.2139\n",
      "Epoch 8 Loss 2.7234 Accuracy 0.2142\n",
      "Epoch 9 Batch 0 Loss 2.5923 Accuracy 0.2198\n",
      "Epoch 9 Batch 50 Loss 2.5825 Accuracy 0.2186\n",
      "Epoch 9 Batch 100 Loss 2.5333 Accuracy 0.2215\n",
      "Epoch 9 Loss 2.5286 Accuracy 0.2216\n",
      "Epoch 10 Batch 0 Loss 2.4519 Accuracy 0.2250\n",
      "Epoch 10 Batch 50 Loss 2.4091 Accuracy 0.2258\n",
      "Epoch 10 Batch 100 Loss 2.3559 Accuracy 0.2289\n",
      "Epoch 10 Loss 2.3513 Accuracy 0.2290\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', 'quel', 'quel', 'porte', '!', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '!', '<end>', '<end>', '<end>', '<end>']\n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(\"good morning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
